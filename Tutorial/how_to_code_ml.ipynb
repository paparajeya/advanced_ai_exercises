{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d692165c",
   "metadata": {},
   "source": [
    "# How to breakdown a complex ML project into manageable steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e9b6e",
   "metadata": {},
   "source": [
    "## STEP 1: Device Check: CPU or GPU\n",
    "This is important to ensure that our code runs on the appropriate hardware, especially if we're using a GPU for acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8830df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "! pip install torch\n",
    "\n",
    "# Clear output here to avoid clutter\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee2c09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Device Check: CPU or GPU\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f140229",
   "metadata": {},
   "source": [
    "## STEP 2: \n",
    "- Now we know about our device, let's focus on the algorithm that we want to implement.\n",
    "- So, let's say we want to implement a simple GANs (Generative Adversarial Networks) using PyTorch.\n",
    "- What I know about GANs: They are consist of two main components: a Generator and a Discriminator.\n",
    "\n",
    "![gan image](https://i.imgur.com/Ed5ZMfR.png)\n",
    "\n",
    "<p align = \"center\">\n",
    "Fig.1 - GAN Architecture  \n",
    "(<a href=\"https://github.com/JamesAllingham/LaTeX-TikZ-Diagrams\">\n",
    "source\n",
    "</a>)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cbe5e",
   "metadata": {},
   "source": [
    "## STEP 3:\n",
    "- Okay, I know the overall architecture of GANs now, but I need to understand how to implement it in PyTorch.\n",
    "- First let's find the dataset that we want to use.\n",
    "- I know Kaggle is a great place to find datasets, so let's search for \"GAN datasets\" on Kaggle.\n",
    "- I found this [link](https://www.kaggle.com/datasets/splcher/animefacedataset) which has a collection of Anime images that we can use to train our GAN.\n",
    "- Let's download the dataset and extract it to a folder named `data`.\n",
    "- But I might need to preprocess the data before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a07ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we need a custom dataset class for anime images.\n",
    "# But I would need some libraries for that.\n",
    "! pip install pillow\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775f0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's import the necessary libraries\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build out custom dataset class for anime images\n",
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"Custom dataset for loading anime images from a directory.\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Get all image files\n",
    "        self.image_files = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            self.image_files.extend(glob(os.path.join(root_dir, '**', ext), recursive=True))\n",
    "        print(f\"Found {len(self.image_files)} images\")\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images.\"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load and return an image at the given index.\n",
    "        Args:\n",
    "            idx (int): Index of the image to retrieve.\n",
    "        Returns:\n",
    "            image (PIL Image or Tensor): Loaded image after applying transforms.\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7471a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I'm using Kaggle, I need to install `kagglehub` to download datasets\n",
    "! pip install kagglehub\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f314b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define the dataset path and download the dataset\n",
    "dataset_path = \"./data\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a90f9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/splcher/animefacedataset?dataset_version_number=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395M/395M [00:14<00:00, 29.3MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets import kagglehub and download the dataset\n",
    "import kagglehub\n",
    "\n",
    "# In order to use kagglehub, we need to set up our Kaggle API credentials.\n",
    "# Follow these steps: https://www.kaggle.com/general/74235\n",
    "# or this instruction: https://github.com/Kaggle/kagglehub\n",
    "# Make sure to upload your `kaggle.json` file to `~/.kaggle/kaggle.json`.\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"splcher/animefacedataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cd2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /home/prashant/.cache/kagglehub/datasets/splcher/animefacedataset/versions/3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dea1b2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image size: (62, 62)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the size of a sample image from the dataset\n",
    "sample_image_path = os.path.join(path, \"images/10000_2004.jpg\")\n",
    "with Image.open(sample_image_path) as img:\n",
    "    print(f\"Sample image size: {img.size}\")  # Output the size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f88fbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets resize it to 64x64 for our GAN model\n",
    "IMG_SIZE = 64\n",
    "\n",
    "# But also at the same time, we need to convert it to a tensor\n",
    "# And also normalize it to the range [-1, 1]\n",
    "\n",
    "# For this we want to use `transforms` from `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e55661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets install torchvision if not already installed\n",
    "! pip install torchvision\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4097412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to import transforms and then define our transformations for the dataset in sequence\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize to 64x64\n",
    "    transforms.ToTensor(),                     # Convert to Tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0826fef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63565 images\n"
     ]
    }
   ],
   "source": [
    "# We have transforms ready, now we can create our dataset instance\n",
    "# That will automatically apply the transforms to each image when accessed - as we already defined in our custom dataset class\n",
    "dataset = AnimeDataset(root_dir=path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06f0be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have to create a DataLoader to load the data in batches\n",
    "# Lets import DataLoader class from torch.utils.data first\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac3b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the dataloader setting batch size, shuffling and num_workers\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = True\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39c259b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to create the train dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "898cd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might want to visualize some images from the dataloader to ensure everything is working fine\n",
    "# Lets install matplotlib if not already installed\n",
    "! pip install matplotlib\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c721e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets visualize some images from the dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "figure = plt.figure(figsize=(14, 14))\n",
    "cols, rows = 7, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "    img = dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"Sample {i}\")\n",
    "    plt.axis(\"off\")\n",
    "    # Convert from [-1,1] to [0,1] for display and transpose for matplotlib\n",
    "    img_display = (img + 1) / 2\n",
    "    img_display = img_display.permute(1, 2, 0)  # CHW to HWC\n",
    "    plt.imshow(img_display)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc5342",
   "metadata": {},
   "source": [
    "## STEP 4: Now we need to build the Generator and Discriminator models.\n",
    "- Let's start with the Generator model.\n",
    "- The Generator takes random noise as input and generates fake images.\n",
    "- We'll use pytorch's `nn.Module` to define our model. More details about `nn.Module` can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "- Let's define the Generator architecture:\n",
    "  - Input: Random noise vector (latent vector)\n",
    "  - Layers: A series of transposed convolutional layers (also known as deconvolutional layers) to upsample the noise vector to the desired image size.\n",
    "  - Output: Generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21ab7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import nn module from torch to build our models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Class\n",
    "# - the constructor will define the layers of the model\n",
    "# - the constructor will take latent_dim as input parameter\n",
    "# - the forward method will define the forward pass of the model\n",
    "# - We need to pass the noise vector to the forward method to generate images\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator Model for Vanilla GAN\"\"\"\n",
    "    def __init__(self, z=10, img_channels=3, feature_map_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z = z\n",
    "        self.img_channels = img_channels\n",
    "        self.feature_map_size = feature_map_size\n",
    "        \n",
    "        # I'll need to define the model architecture here\n",
    "        # pytorch has a module called `nn.Sequential` which allows us to stack layers sequentially\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is the latent vector Z\n",
    "            # We will use ConvTranspose2d layers to upsample the noise vector\n",
    "            # Kernel size of 4, stride of 2, padding of 1 -> doubles the spatial dimensions\n",
    "            # First layer will take the noise vector and produce a feature map of size (feature_map_size*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(z, feature_map_size * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            \n",
    "            # After each ConvTranspose2d layer, we will use BatchNorm and ReLU activation for non-linearity\n",
    "            nn.BatchNorm2d(feature_map_size * 8),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (feature_map_size*8) x 4 x 4\n",
    "            \n",
    "            # Layer 2\n",
    "            # This will produce a feature map of size (feature_map_size*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(feature_map_size * 8, feature_map_size * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 4),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (feature_map_size*4) x 8 x 8\n",
    "            \n",
    "            # Layer 3\n",
    "            # This will produce a feature map of size (feature_map_size*2) x\n",
    "            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (feature_map_size*2) x 16 x 16\n",
    "            \n",
    "            # Layer 4\n",
    "            # This will produce a feature map of size (feature_map_size) x 32 x 32\n",
    "            nn.ConvTranspose2d(feature_map_size * 2, feature_map_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (feature_map_size) x 32 x 32\n",
    "            \n",
    "            # Final Layer\n",
    "            # This will produce the final image of size (img_channels) x 64 x 64\n",
    "            nn.ConvTranspose2d(feature_map_size, img_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            \n",
    "            # Using Tanh activation to ensure the output is in the range [-1, 1] to match the normalized image range\n",
    "            nn.Tanh()\n",
    "            # Output size: (img_channels) x 64 x 64\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"Forward pass of the generator.\n",
    "        Args:\n",
    "            z (Tensor): Input noise tensor of shape (batch_size, latent_dim, 1, 1)\n",
    "        Returns:\n",
    "            Tensor: Generated images of shape (batch_size, img_channels, IMG_SIZE, IMG_SIZE)\n",
    "        \"\"\"\n",
    "        return self.model(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9566af2",
   "metadata": {},
   "source": [
    "### Question: What should be the architecture of the Generator model if we want to generate 512x512 images from a 100-dimensional noise vector?\n",
    "\n",
    "- To generate 512x512 images from a 100-dimensional noise vector, we can use a series of transposed convolutional layers to progressively upsample the noise vector. Here's a possible architecture for the Generator model:\n",
    "\n",
    "- Input: 100-dimensional noise vector (z)\n",
    "- Layer 1: Transposed Convolutional Layer\n",
    "  - Input Channels: 100\n",
    "  - Output Channels: 512\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 1\n",
    "  - Padding: 0\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (512, 4, 4)\n",
    "\n",
    "- Layer 2: Transposed Convolutional Layer\n",
    "  - Input Channels: 512\n",
    "  - Output Channels: 256\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (256, 8, 8)\n",
    "\n",
    "- Layer 3: Transposed Convolutional Layer\n",
    "  - Input Channels: 256\n",
    "  - Output Channels: 128\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (128, 16, 16)\n",
    "\n",
    "- Layer 4: Transposed Convolutional Layer\n",
    "  - Input Channels: 128\n",
    "  - Output Channels: 64\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (64, 32, 32)\n",
    "\n",
    "- Layer 5: Transposed Convolutional Layer\n",
    "  - Input Channels: 64\n",
    "  - Output Channels: 32\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (32, 64, 64)\n",
    "\n",
    "- Layer 6: Transposed Convolutional Layer\n",
    "  - Input Channels: 32\n",
    "  - Output Channels: 16\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (16, 128, 128)\n",
    "\n",
    "- Layer 7: Transposed Convolutional Layer\n",
    "  - Input Channels: 16\n",
    "  - Output Channels: 8\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: ReLU\n",
    "  - Output Size: (8, 256, 256)\n",
    "\n",
    "- Layer 8: Transposed Convolutional Layer\n",
    "  - Input Channels: 8\n",
    "  - Output Channels: 3 (for RGB images)\n",
    "  - Kernel Size: 4\n",
    "  - Stride: 2\n",
    "  - Padding: 1\n",
    "  - Activation: Tanh (to ensure output values are in the range [-1, 1])\n",
    "  - Output Size: (3, 512, 512)\n",
    "  \n",
    "### Don't forget:\n",
    "- Forward Method:\n",
    "  - The forward method will define how the input noise vector is passed through the layers to produce the output image.\n",
    "  - We need to pass the noise vector to the forward method to generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "029c909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets move back to the models we were building.\n",
    "# After the Generator, we need to build the Discriminator model.\n",
    "# Discriminator Class will be similar to the Generator but in reverse.\n",
    "\n",
    "# Discriminator Class\n",
    "# - the constructor will define the layers of the model\n",
    "# - the constructor will take img_channels as input parameter\n",
    "# - the forward method will define the forward pass of the model\n",
    "# - We need to pass the image tensor to the forward method to classify real/fake\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator Model for Vanilla GAN\"\"\"\n",
    "    def __init__(self, img_channels=3, feature_map_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_channels = img_channels\n",
    "        self.feature_map_size = feature_map_size\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Input is the image tensor (img_channels) x 64 x 64\n",
    "            # We will use Conv2d layers to downsample the image\n",
    "            # Kernel size of 4, stride of 2, padding of 1 -> halves the spatial dimensions\n",
    "            # First layer will take the image and produce a feature map of size (feature_map_size) x 32 x 32\n",
    "            nn.Conv2d(img_channels, feature_map_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (feature_map_size) x 32 x 32\n",
    "            \n",
    "            # Layer 2\n",
    "            # This will produce a feature map of size (feature_map_size*2) x 16 x 16\n",
    "            nn.Conv2d(feature_map_size, feature_map_size * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (feature_map_size*2) x 16 x 16\n",
    "            \n",
    "            # Layer 3\n",
    "            # This will produce a feature map of size (feature_map_size*4) x 8 x 8\n",
    "            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (feature_map_size*4) x 8 x 8\n",
    "            \n",
    "            # Layer 4\n",
    "            # This will produce a feature map of size (feature_map_size*8) x 4 x 4\n",
    "            nn.Conv2d(feature_map_size * 4, feature_map_size * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_map_size * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (feature_map_size*8) x 4 x 4\n",
    "            nn.Conv2d(feature_map_size * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()  # Output a probability between 0 and 1\n",
    "            # Output size: 1 x 1 x 1 (real/fake probability)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \"\"\"Forward pass of the discriminator.\n",
    "        Args:\n",
    "            img (Tensor): Input image tensor of shape (batch_size, img_channels, IMG_SIZE, IMG_SIZE)\n",
    "        Returns:\n",
    "            Tensor: Probability tensor of shape (batch_size, 1, 1, 1) indicating real/fake\n",
    "        \"\"\"\n",
    "        return self.model(img).view(-1, 1).squeeze(1)  # Flatten output to (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70fefa",
   "metadata": {},
   "source": [
    "# STEP 5: We now need to create objects of the Generator and Discriminator classes and move them to the appropriate device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5243b577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Objects of Generator and Discriminator classes\n",
    "# - We now need to create objects of the Generator and Discriminator classes and move them to the appropriate device (CPU or GPU).\n",
    "# - We will also print the model summaries to verify everything is correct\n",
    "# - We will use a latent dimension of 100 for the Generator\n",
    "latent_dim = 100\n",
    "img_channels = 3\n",
    "feature_map_size = 64\n",
    "\n",
    "G = Generator(z=latent_dim, img_channels=img_channels, feature_map_size=feature_map_size)\n",
    "G.to(device) # Move Generator object to device\n",
    "\n",
    "D = Discriminator(img_channels=img_channels, feature_map_size=feature_map_size)\n",
    "D.to(device) # Move Discriminator object to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf3dab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print the model summaries, we can use the `summary` function from `torchsummary` package\n",
    "! pip install torchsummary\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28e44022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import summary function from torchsummary\n",
    "import numpy\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Generator summary\n",
    "summary(G, (latent_dim, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator summary\n",
    "summary(D, (3, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9153131",
   "metadata": {},
   "source": [
    "## STEP 6: Training the GAN\n",
    "- Now that we have defined our Generator and Discriminator models, we need to train them.\n",
    "- The training process involves alternating between training the Discriminator and the Generator.\n",
    "\n",
    "### Step 6.1: Train the Discriminator\n",
    "The discriminator's goal is to get better at distinguishing real images from fakes. Its total loss is the sum of its performance on real and fake images.\n",
    "$$ \\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^{m} [\\log D(x^{(i)}) + \\log(1 - D(G(z^{(i)})))] $$\n",
    "- We feed it a batch of **real images** and train it to classify them as `real` (label=1).\n",
    "- We then feed it a batch of **fake images** (generated by the generator) and train it to classify them as `fake` (label=0).\n",
    "\n",
    "### Step 6.2: Train the Generator\n",
    "The generator's goal is to fool the discriminator. It wants the discriminator to classify its fake images as real.\n",
    "$$ \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^{m} \\log(D(G(z^{(i)}))) $$\n",
    "- We generate a new batch of **fake images**.\n",
    "- We pass them through the discriminator, but this time, we use `real` labels (label=1).\n",
    "- We calculate the loss and backpropagate it to update **only the generator's weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897dc42",
   "metadata": {},
   "source": [
    "### Training Loop/Algorithm\n",
    "- We would need to define the number of epochs - how many times we want to go through the entire dataset again and again.\n",
    "- For each epoch, we would iterate through the dataset in batches - for each batch, we would perform the steps to train the discriminator and generator.\n",
    "- But before that, we need to define the loss function and optimizers for both models. This is typically done using Binary Cross Entropy Loss and Adam optimizer.\n",
    "\n",
    "#### Bianry Cross Entropy Loss\n",
    "- We can use `nn.BCELoss()` from PyTorch to define the binary cross-entropy loss function.\n",
    "\n",
    "#### Adam Optimizer\n",
    "- We can use `torch.optim.Adam` to define the Adam optimizer for both the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4669cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss function and Optimizers\n",
    "# - We will use Binary Cross Entropy Loss for both models\n",
    "# - We will use Adam optimizer for both models\n",
    "# - We will set the learning rate to 0.0001 and betas to (0.5, 0.999)\n",
    "# - Betas are hyperparameters for the Adam optimizer that control the exponential decay rates of moving averages of past gradients and squared gradients.\n",
    "# - Beta1 (0.5) is used for the first moment estimate (mean of gradients) and helps to smooth out the updates.\n",
    "# - Beta2 (0.999) is used for the second moment estimate (uncentered variance of gradients) and helps to stabilize the training by reducing the variance of the updates.\n",
    "# - These values are commonly used in GAN training to help with convergence and stability.\n",
    "# - We will define separate optimizers for the Generator and Discriminator\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.0001\n",
    "betas = (0.5, 0.999)\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=betas)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f046c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop/Algorithm\n",
    "# - We would need to define the number of epochs - how many times we want to go through the entire dataset again and again.\n",
    "# - For each epoch, we would iterate through the dataset in batches - for each batch, we would perform the steps to train the discriminator and generator.\n",
    "epochs = 100\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "# Epoch loop\n",
    "for i in range(epochs):\n",
    "    # Let's show the progress of training\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {i+1}/{epochs}\")\n",
    "    for batch_idx, real_images in enumerate(pbar):\n",
    "        # STEP 1: Get the batch size\n",
    "        batch_size = real_images.size(0)\n",
    "        # STEP 2: Move real images to device\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # So we have real images in the range [-1, 1] as we normalized them earlier\n",
    "        # We need to create labels for real and fake images\n",
    "        # Real images are labeled as 1 and fake images are labeled as 0\n",
    "        # We will use these labels to compute the loss for the discriminator\n",
    "\n",
    "        \"\"\"\n",
    "        Train Discriminator\n",
    "        \"\"\"\n",
    "        optimizer_D.zero_grad() # Zero the gradients for the discriminator\n",
    "        \n",
    "        # Discriminator will be trained on both real and fake images\n",
    "        # What we do as steps:\n",
    "        # - Label real images as 1\n",
    "        # - Forward pass real images through D - get D's output\n",
    "        # - Calculate loss on real images\n",
    "        # Real images\n",
    "        labels_real = torch.ones(batch_size, device=device) # Loaded dataset images are real images - hence labeled as 1\n",
    "        output_real = D(real_images) # Forward pass real batch through D\n",
    "        loss_real = criterion(output_real, labels_real) # Calculate loss on all-real batch\n",
    "        \n",
    "        # Now we need to generate fake images using the generator\n",
    "        # - We will sample random noise vectors from a normal distribution\n",
    "        # - We will pass these noise vectors to the generator to generate fake images\n",
    "        # - We will label these fake images as 0\n",
    "        # - We will forward pass these fake images through the discriminator to get D's output\n",
    "        # - We will calculate the loss on fake images\n",
    "        # Fake images\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "        fake_images = G(noise)\n",
    "        labels_fake = torch.zeros(batch_size, device=device)\n",
    "        output_fake = D(fake_images.detach())  # Detach to avoid training G on these labels\n",
    "        loss_fake = criterion(output_fake, labels_fake)\n",
    "        \n",
    "        # So in the GAN literature, the discriminator's loss is typically computed as the sum of the losses on real and fake images.\n",
    "        # That way, the discriminator is penalized for misclassifying both real and fake images.\n",
    "        # Total Discriminator loss\n",
    "        loss_D = loss_real + loss_fake\n",
    "        \n",
    "        # Once the loss is computed, we perform backpropagation and update the discriminator's weights\n",
    "        loss_D.backward()\n",
    "        # We then update the weights of the discriminator using the optimizer\n",
    "        optimizer_D.step()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Train Generator\n",
    "        \"\"\"\n",
    "        # We want to train the generator to fool the discriminator\n",
    "        # At the same time we want to keep the last trained discriminator's weights fixed\n",
    "        optimizer_G.zero_grad() # Zero the gradients for the generator\n",
    "        \n",
    "        # We want the generator to fool the discriminator\n",
    "        # Generator will fool the discriminator if the discriminator classifies the fake images as real\n",
    "        # So we will label the fake images as real (1) and compute the loss\n",
    "        # We will then backpropagate this loss through the generator to update its weights\n",
    "        labels_gen = torch.ones(batch_size, device=device)  # We want the fake images to be classified as real\n",
    "        output_gen = D(fake_images) # Forward pass fake images through D\n",
    "        loss_G = criterion(output_gen, labels_gen) # Calculate loss on fake images\n",
    "        \n",
    "        # Backpropagate the loss through the generator\n",
    "        loss_G.backward()\n",
    "        # Update the weights of the generator using the optimizer\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Save the losses for plotting later\n",
    "        G_losses.append(loss_G.item())\n",
    "        D_losses.append(loss_D.item())\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.set_postfix({\"Loss D\": loss_D.item(), \"Loss G\": loss_G.item()})\n",
    "    pbar.close()\n",
    "\n",
    "    print(f\"Epoch [{i+1}/{epochs}]  Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba92c57",
   "metadata": {},
   "source": [
    "## STEP 7: Monitoring Progress\n",
    "- During training, it's important to monitor the progress of both the generator and discriminator.\n",
    "- We can do this by printing the losses of both models after each epoch.\n",
    "- Additionally, we can generate and save some sample images from the generator at regular intervals (e.g., every 10 epochs) to visually inspect the quality of the generated images over time.\n",
    "- We can use libraries like Matplotlib to display and save these images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078aecc4",
   "metadata": {},
   "source": [
    "### Training Loss Visualization\n",
    "\n",
    "Plotting the losses helps us understand the dynamics of the adversarial training. \n",
    "- **Discriminator Loss (D)**: Ideally, this should hover around 0.5. If it goes to 0, the generator isn't learning.\n",
    "- **Generator Loss (G)**: We want to see this loss decrease, indicating the generator is getting better at fooling the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save the trained models for future use\n",
    "torch.save(G.state_dict(), \"generator.pth\")\n",
    "torch.save(D.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196510d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate some images using the trained generator\n",
    "# We will sample random noise vectors and pass them to the generator to generate images\n",
    "# We will then visualize these generated images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Set the generator to evaluation mode\n",
    "G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sample some random noise vectors\n",
    "num_samples = 10\n",
    "noise = torch.randn(num_samples, latent_dim, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    generated_images = G(noise).cpu()  # Generate images and move to CPU\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(14, 14))\n",
    "cols, rows = 5, 2\n",
    "for i in range(1, cols * rows + 1):\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"Generated {i}\")\n",
    "    plt.axis(\"off\")\n",
    "    # Convert from [-1,1] to [0,1] for display and transpose for matplotlib\n",
    "    img_display = (generated_images[i-1] + 1) / 2\n",
    "    img_display = img_display.permute(1, 2, 0)  # CHW to HWC\n",
    "    plt.imshow(img_display)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
